{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üîç 1. What is Pretraining?\n",
        "\n",
        "Pretraining is the **initial training phase** where a language model learns general language understanding by processing vast amounts of unlabeled text.\n",
        "\n",
        "-  Objective: Learn grammar, facts, and language structure.\n",
        "-  Dataset: Very large and diverse (Wikipedia, books, Common Crawl, etc.)\n",
        "-  Example Objective:\n",
        " - Masked Language Modeling (BERT)\n",
        " - Causal Language Modeling (GPT)"
      ],
      "metadata": {
        "id": "ZCO3LJU0GKS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Techniques Used:\n",
        "- Self-supervised learning\n",
        "- Next token prediction (GPT) or masked word prediction (BERT)\n"
      ],
      "metadata": {
        "id": "db2U-isbGlll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ 2. What is Fine-tuning?\n",
        "\n",
        "Fine-tuning adapts a pretrained model to a **specific downstream task** (e.g., sentiment analysis, summarization, Q&A).\n",
        "\n",
        "- Objective: Improve performance on a **task-specific** dataset\n",
        "- Dataset: Small and labeled (e.g., IMDb movie reviews, medical Q&A, etc.)\n",
        "- Can be supervised or reinforcement learning-based (e.g., RLHF for ChatGPT)"
      ],
      "metadata": {
        "id": "d_smKu6CGpVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Key Differences\n",
        "\n",
        "| Feature              | Pretraining                           | Fine-tuning                          |\n",
        "|----------------------|----------------------------------------|---------------------------------------|\n",
        "| **Goal**             | General language learning              | Task-specific adaptation              |\n",
        "| **Data**             | Huge, unlabeled, diverse               | Small, labeled, task-specific         |\n",
        "| **Cost**             | Extremely compute-intensive            | Comparatively lightweight             |\n",
        "| **Reusability**      | Model can be reused for many tasks     | Specific to the fine-tuned task       |\n"
      ],
      "metadata": {
        "id": "MRbbUMlVGuvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Real-world Example\n",
        "\n",
        "- **BERT** is pretrained on BooksCorpus + Wikipedia with MLM\n",
        "- Later **fine-tuned** on:\n",
        "  - SQuAD for Q&A\n",
        "  - SST-2 for Sentiment Classification"
      ],
      "metadata": {
        "id": "1DpHe0rHGzVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Fine-tuning Variants\n",
        "\n",
        "- **Full fine-tuning**: All parameters updated\n",
        "- **Feature-based**: Use embeddings as input to other models\n",
        "- **Adapter tuning / LoRA**: Only a small part of the model is updated\n"
      ],
      "metadata": {
        "id": "7rnCWnizG6bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Summary\n",
        "\n",
        "- Pretraining = Build general intelligence from massive data  \n",
        "- Fine-tuning = Customize intelligence for a specific job  \n",
        "- Pretrained LLMs are like **generalists**, fine-tuning turns them into **specialists**."
      ],
      "metadata": {
        "id": "u9MSXuKjG_Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E094ZMm4GXv4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}