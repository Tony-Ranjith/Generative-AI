{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîÑ Encoders vs Decoders in the Transformer Architecture\n",
        "\n",
        "In the original Transformer model (from *‚ÄúAttention is All You Need‚Äù*), the architecture is split into two major parts:\n",
        "\n",
        "- **Encoder**: Processes and understands the input sequence.\n",
        "- **Decoder**: Generates the output sequence, one token at a time.\n",
        "\n",
        "Understanding their roles is crucial before diving into models like BERT, GPT, and T5.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What is an Encoder?\n",
        "\n",
        "The **Encoder** takes the input tokens and converts them into contextualized embeddings. These embeddings carry rich information about each token and its relationship with others.\n",
        "\n",
        "Each encoder layer consists of:\n",
        "- Multi-head self-attention\n",
        "- Feed-forward neural network\n",
        "- Add & LayerNorm (residual connection + normalization)\n",
        "\n",
        "‚úÖ **Used in**: BERT, RoBERTa, DistilBERT (Encoder-only models)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è What is a Decoder?\n",
        "\n",
        "The **Decoder** takes the encoder‚Äôs output and generates the final output tokens (like translated text or the next word in a sentence).\n",
        "\n",
        "Each decoder layer includes:\n",
        "- **Masked** multi-head self-attention (only sees past tokens)\n",
        "- Cross-attention (attends to encoder output)\n",
        "- Feed-forward network\n",
        "- Add & LayerNorm\n",
        "\n",
        "‚úÖ **Used in**: GPT, GPT-2, GPT-3 (Decoder-only models), T5 (Encoder-Decoder model)\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Differences\n",
        "\n",
        "| Feature | Encoder | Decoder |\n",
        "|--------|---------|---------|\n",
        "| Input | Full input sequence | Previously generated tokens |\n",
        "| Attention | Self-attention | Masked self-attention + cross-attention |\n",
        "| Use Case | Understanding | Generation |\n",
        "| Sees Full Input? | Yes | No (only past tokens) |\n",
        "| Common Models | BERT, RoBERTa | GPT, LLaMA |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Real-World Examples\n",
        "\n",
        "- **BERT** (Encoder-only): Great for tasks like classification, sentence similarity, Q&A.\n",
        "- **GPT** (Decoder-only): Great for tasks like text generation, story writing, coding.\n",
        "- **T5 / BART** (Encoder-Decoder): Ideal for translation, summarization, question generation.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Summary\n",
        "\n",
        "- Encoders = Understand input\n",
        "- Decoders = Generate output\n",
        "- Encoder-decoder combo = Flexible and powerful for seq-to-seq tasks\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DWpxeZvntfC2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycruWcxYtUwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}